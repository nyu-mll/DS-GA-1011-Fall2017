{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Things\n",
    "We're using SST once again. Load the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home = '../data/trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify your implementation, we'll use a fixed unrolling length of 20. This means that we'll have to expand each sentence into a sequence of 21 word indices. In the conversion process, we'll mark the start of each sentence with a special word symbol `<S>`, mark the end of each sentence (if it occurs within the first 21 words) with a special word symbol `</S>`, mark extra tokens after `</S>` with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "max_seq_length = 20\n",
    "\n",
    "def sentence_to_padded_index_sequence(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    START = \"<S>\"\n",
    "    END = \"</S>\"\n",
    "    END_PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    SEQ_LEN = max_seq_length + 1\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    # Only enter word into vocabulary if it appears > 25 times. Add special symbols to vocabulary.\n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 25]) \n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [START, END, END_PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['index_sequence'] = torch.zeros((SEQ_LEN))\n",
    "            \n",
    "            token_sequence = [START] + tokenize(example['text']) + [END]\n",
    "\n",
    "            for i in range(SEQ_LEN):\n",
    "                if i < len(token_sequence):\n",
    "                    if token_sequence[i] in word_indices:\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                else:\n",
    "                    index = word_indices[END_PADDING]\n",
    "                example['index_sequence'][i] = index\n",
    "                \n",
    "            example['target_sequence'] = example[\"index_sequence\"][1:]\n",
    "            example['index_sequence'] = example['index_sequence'].long().view(1,-1)\n",
    "            example['target_sequence'] = example['target_sequence'].long().view(1,-1)\n",
    "            \n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's batchify data and define the evaluation metric,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the iterator we'll use during training. \n",
    "# It's a generator that gives you one batch at a time.\n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    targets = []\n",
    "    for d in batch:\n",
    "        vectors.append(d[\"index_sequence\"])\n",
    "        targets.append(d[\"target_sequence\"])\n",
    "    return vectors, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model time!\n",
    "\n",
    "Now, using the starter code and hyperparameter values provided below, implement an LSTM language model with dropout on the non-recurrent connections. `lstm_step` is provided for you, you need to complete the forward function with a decoder. \n",
    "\n",
    "We won't be evaluating our model in the conventional way (perplexity on a held-out test set) for a few reasons: to save time, because we have no baseline to compare against, and because overfitting the training set is a less immediate concern with these models than it was with sentence classifiers. Instead, we'll use the value of the loss function to make sure that the model is converging as expected, and we'll use samples drawn from the model to qualitatively evaluate it.\n",
    "\n",
    "So you will also need to complete the `sample` function such that when it is called, and it is provided with the starting symbol `<S>`, then is generates a sentence that is 20 tokens long (20 being our max sequence length). You should calucalte word weights with a softmax but make sure to use the softmax trick of subtracting the maximum value,\n",
    "$$ weight_i = \\frac{\\exp(x_i - \\max_i(x))}{\\sum_i(\\exp(x_i - max_i(x)))} $$\n",
    "Then use these weights to sample from the distribution and generate tokens.\n",
    "\n",
    "Hint: you will need to use `torch.multinomial()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embedding_dim, hidden_size, batch_size):\n",
    "        super(RNNLM, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.linear_f = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.linear_i = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.linear_ctilde = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.linear_o = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.init_weights()\n",
    "                \n",
    "    def forward(self, x, hidden, c):\n",
    "        x_emb = self.encoder(x)\n",
    "        embs = torch.chunk(x_emb, x_emb.size()[1], 1)  \n",
    "        hiddens = []\n",
    "        logits = []\n",
    "        for i in range(self.seq_len - 1):\n",
    "            hidden, c = self.lstm_step(embs[i].squeeze(), hidden, c)\n",
    "            hiddens.append(hidden)\n",
    "            hidden_drop = self.dropout(hidden)\n",
    "            logit = self.decoder(hidden_drop)\n",
    "            logits.append(logit)\n",
    "            \n",
    "        return hiddens, logits  \n",
    "        \n",
    "    def sample(self, x_start, hidden, c):\n",
    "        indices = [int(x_start.data.numpy())]\n",
    "        for i in range(self.seq_len - 1):\n",
    "            x_emb = self.encoder(x_start)\n",
    "            hidden, c = self.lstm_step(x_emb.view(1,-1), hidden, c) \n",
    "            logits = self.decoder(hidden)\n",
    "            \n",
    "            logits = logits.squeeze().data\n",
    "            exp_logits = torch.exp(logits - torch.max(logits))\n",
    "            word_weights = exp_logits / exp_logits.sum()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            x_start.data.fill_(word_idx)\n",
    "            indices.append(word_idx)\n",
    "            \n",
    "        return indices\n",
    "        \n",
    "    def lstm_step(self, emb, hid, c_t):\n",
    "        emb_drop = self.dropout(emb)\n",
    "        combined = torch.cat((hid, emb_drop), 1)\n",
    "        f = F.sigmoid(self.linear_f(combined))\n",
    "        i = F.sigmoid(self.linear_i(combined))\n",
    "        c_tilde = F.tanh(self.linear_ctilde(combined))\n",
    "        c_t = f * c_t + i * c_tilde\n",
    "        o = F.sigmoid(self.linear_o(combined))\n",
    "        hid = o * F.tanh(c_t)\n",
    "        return hid, c_t\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_f, self.linear_i, self.linear_ctilde, self.linear_o, self.decoder]\n",
    "        em_layer = [self.encoder]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)\n",
    "                \n",
    "    def init_hidden(self, bsz):\n",
    "        h0 = Variable(torch.zeros(bsz, self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(bsz, self.hidden_size))\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training loop,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(batch_size, vocab_size, num_epochs, display_freq, model, criterion, optim, training_iter):\n",
    "    step = 1\n",
    "    epoch = 1\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()      \n",
    "        inputs, targets = get_batch(next(training_iter)) \n",
    "        inputs = Variable(torch.stack(inputs).squeeze(), volatile=False) # batch_size,  seq_len\n",
    "        targets = Variable(torch.stack(targets).squeeze().view(-1)) # batch_size,seq_len --> flat, b*s\n",
    "    \n",
    "        model.zero_grad()     \n",
    "        hidden, c_t = model.init_hidden(batch_size)\n",
    "        hiddens, output = model(inputs, hidden, c_t)\n",
    "        output_flat = torch.stack(output, dim=1).view(-1, vocab_size) # batch_size, seq_len, hid --> b*s,h\n",
    "\n",
    "        lossy = criterion(output_flat, targets)\n",
    "        lossy.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if step % total_batches == 0:\n",
    "            if epoch % display_freq == 0:\n",
    "                hidden, c_t = model.init_hidden(1)\n",
    "                input = Variable(torch.zeros(1, 1).long(), volatile=True)\n",
    "                indices = model.sample(input, hidden, c_t)\n",
    "                words = [indices_to_words[index] for index in indices]\n",
    "                print(\"Epoch: {}; Loss: {}; Sample: {}\".format(epoch, lossy.data[0], ' '.join(words)))\n",
    "            epoch += 1\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and sample!\n",
    "\n",
    "Now train model with the suggested hyperparameters.\n",
    "\n",
    "Once you're confident your model is doing what you want, let it run for the full 350 epochs. This will take some timeâ€”likely between ten and forty minutes, so do it at home if you don't have time during the class. We train the model for a fairly long time because these small improvements in cost correspond to fairly large improvements in sample quality.\n",
    "\n",
    "Samples from a trained models should have coherent portions, but they will not resemble interpretable English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Loss: 5.46087121963501; Sample: <S> nearly told fascinating making lack shows silly films always ride gets history were lacks gags no feature enjoyable those overall\n",
      "Epoch: 2; Loss: 4.456007480621338; Sample: <S> looking <UNK> <UNK> <UNK> us personal film me completely <PAD> summer too goes 'd <PAD> michael acting off <UNK> <PAD>\n",
      "Epoch: 3; Loss: 4.263888835906982; Sample: <S> as <UNK> humor actually you come <PAD> violence <UNK> <PAD> comes <UNK> <UNK> role <PAD> <PAD> <PAD> <PAD> <UNK> <PAD>\n",
      "Epoch: 4; Loss: 3.9805500507354736; Sample: <S> none without little charming characters wrong <PAD> <PAD> interest <PAD> <UNK> intelligent sure <UNK> <PAD> <UNK> <UNK> <PAD> <PAD> <UNK>\n",
      "Epoch: 5; Loss: 3.738694429397583; Sample: <S> <PAD> mr. <PAD> the minutes study <PAD> <PAD> character a engaging <PAD> the <PAD> a , <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 6; Loss: 3.5647494792938232; Sample: <S> play can whole is sex <PAD> , <PAD> <UNK> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 7; Loss: 3.539154052734375; Sample: <S> theater ever but emotional one <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 8; Loss: 3.4966678619384766; Sample: <S> better <UNK> and best for will <UNK> brilliant of and </S> leave often <UNK> ca to the , such ,\n",
      "Epoch: 9; Loss: 3.4600284099578857; Sample: <S> tone while <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 10; Loss: 3.34553599357605; Sample: <S> ` rarely old storytelling <UNK> <UNK> <UNK> everything themselves <UNK> <UNK> , as <UNK> <UNK> <UNK> <UNK> <UNK> . <PAD>\n",
      "Epoch: 11; Loss: 3.5587539672851562; Sample: <S> <UNK> people <UNK> 's to <UNK> <UNK> <UNK> a , <UNK> and is delivers and movie de kids <UNK> it\n",
      "Epoch: 12; Loss: 3.4736199378967285; Sample: <S> the flick strong <UNK> <UNK> <UNK> but <UNK> the <UNK> or <UNK> at a <UNK> some or 's <UNK> <UNK>\n",
      "Epoch: 13; Loss: 3.4138731956481934; Sample: <S> need no 's got and , at the the <UNK> in of a effort . this . <PAD> <PAD> <PAD>\n",
      "Epoch: 14; Loss: 3.4711813926696777; Sample: <S> ca her slow viewers social audience to a performances <UNK> <UNK> that <UNK> <UNK> a <UNK> <UNK> it the <UNK>\n",
      "Epoch: 15; Loss: 3.37949800491333; Sample: <S> <UNK> an <UNK> should this , <UNK> <UNK> <UNK> <UNK> but <UNK> full a film <UNK> <UNK> <UNK> as make\n",
      "Epoch: 16; Loss: 3.3097004890441895; Sample: <S> it the <UNK> another above more , whether with times , and of to <UNK> next <UNK> to <UNK> and\n",
      "Epoch: 17; Loss: 3.4788730144500732; Sample: <S> what <UNK> through love only <UNK> a <UNK> a effort 's <UNK> <UNK> exercise , under earnest for a own\n",
      "Epoch: 18; Loss: 3.2816054821014404; Sample: <S> at <PAD> </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 19; Loss: 3.303784132003784; Sample: <S> lacks <UNK> <UNK> <UNK> its of <UNK> an already <UNK> with touching these <UNK> <UNK> of <UNK> <UNK> back <UNK>\n",
      "Epoch: 20; Loss: 3.3180155754089355; Sample: <S> otherwise , of <UNK> , the <UNK> but , it watching <UNK> and <UNK> , <UNK> to that characters has\n",
      "Epoch: 21; Loss: 3.4303383827209473; Sample: <S> not <UNK> n't <UNK> better kind <UNK> tragedy of <UNK> <UNK> two left <UNK> <UNK> a in <UNK> <UNK> <UNK>\n",
      "Epoch: 22; Loss: 3.3622775077819824; Sample: <S> sort the drama <UNK> <UNK> of out for to but <UNK> . woman </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 23; Loss: 3.3338959217071533; Sample: <S> <UNK> , time reality 're , but his <UNK> , people of need around . of <UNK> 's , only\n",
      "Epoch: 24; Loss: 3.260676622390747; Sample: <S> must <UNK> a the script <UNK> on while <UNK> to is home enough <UNK> in <UNK> help <UNK> , the\n",
      "Epoch: 25; Loss: 3.2635974884033203; Sample: <S> seems <UNK> <UNK> . </S> </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 26; Loss: 3.2533602714538574; Sample: <S> has is the <UNK> <UNK> <UNK> first <UNK> <UNK> of <UNK> an is <UNK> hard difficult it you enough n't\n",
      "Epoch: 27; Loss: 3.3473658561706543; Sample: <S> seeing <UNK> the in heart the if and made smart this <UNK> of it for , the action <UNK> for\n",
      "Epoch: 28; Loss: 3.3951332569122314; Sample: <S> <UNK> suspense who little be n't <UNK> <UNK> everyone strong , for it <UNK> of . in <UNK> is never\n",
      "Epoch: 29; Loss: 3.1313912868499756; Sample: <S> must in the not this to <UNK> of <UNK> , . <UNK> really time three it on men enough <UNK>\n",
      "Epoch: 30; Loss: 3.150164842605591; Sample: <S> <UNK> from tale as the <UNK> ending of <UNK> <UNK> <UNK> a any , end story <UNK> life -- and\n",
      "Epoch: 31; Loss: 3.1706583499908447; Sample: <S> keep : <UNK> you <UNK> the story one . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 32; Loss: 3.271926164627075; Sample: <S> bland with satisfying be family <UNK> , <UNK> more <UNK> pretentious as which <UNK> <UNK> <UNK> . </S> <PAD> <PAD>\n",
      "Epoch: 33; Loss: 3.3253417015075684; Sample: <S> <UNK> <UNK> which <UNK> , <UNK> want . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 34; Loss: 3.2100746631622314; Sample: <S> <UNK> <UNK> the a ultimately their <UNK> is mostly a barely <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 35; Loss: 3.1775238513946533; Sample: <S> <UNK> this <UNK> dumb <UNK> right love out -- <UNK> both tone the <UNK> some that the <UNK> <UNK> <UNK>\n",
      "Epoch: 36; Loss: 3.303417682647705; Sample: <S> <UNK> none perfect viewers n't <UNK> fans of <UNK> worth face <UNK> <UNK> the <UNK> , <UNK> rather a <UNK>\n",
      "Epoch: 37; Loss: 3.1554629802703857; Sample: <S> so <UNK> , us <UNK> , the <UNK> , play in <UNK> characters , if it and to people full\n",
      "Epoch: 38; Loss: 3.2831673622131348; Sample: <S> who classic 's mess <UNK> <UNK> are back <UNK> the <UNK> <UNK> the not honest . </S> <PAD> <PAD> <PAD>\n",
      "Epoch: 39; Loss: 3.278886318206787; Sample: <S> <UNK> 's <UNK> <UNK> <UNK> , <UNK> <UNK> who to 's <UNK> cinematic <UNK> the <UNK> enough -lrb- , an\n",
      "Epoch: 40; Loss: 3.1943259239196777; Sample: <S> <UNK> it enough `` the <UNK> serious moments but <UNK> film <UNK> say , the film you <UNK> and <UNK>\n",
      "Epoch: 41; Loss: 3.0390710830688477; Sample: <S> his point and <UNK> <UNK> <UNK> a <UNK> made <UNK> seem experience <UNK> , <UNK> <UNK> <UNK> real <UNK> long\n",
      "Epoch: 42; Loss: 3.0675809383392334; Sample: <S> . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 43; Loss: 3.229207992553711; Sample: <S> production actor matter <UNK> but 's <UNK> <UNK> point <UNK> , only <UNK> story <UNK> <UNK> in <UNK> , <UNK>\n",
      "Epoch: 44; Loss: 3.3704750537872314; Sample: <S> beautifully the <UNK> n't <UNK> what n't <UNK> an the <UNK> surprising . </S> </S> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 45; Loss: 3.1935200691223145; Sample: <S> , sex n't a <UNK> but girl <UNK> of : that be film 's of <UNK> of <UNK> <UNK> </S>\n",
      "Epoch: 46; Loss: 3.134403705596924; Sample: <S> they is charming <UNK> days <UNK> <UNK> down that up <UNK> way to <UNK> really to <UNK> who and and\n",
      "Epoch: 47; Loss: 3.113516330718994; Sample: <S> with thought the <UNK> <UNK> , <UNK> <UNK> with book and <UNK> <UNK> it barely <UNK> real a <UNK> you\n",
      "Epoch: 48; Loss: 3.1610021591186523; Sample: <S> <UNK> <UNK> and <UNK> shows , i and <UNK> well . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 49; Loss: 3.2947604656219482; Sample: <S> production tired comedy , what to should instead of <UNK> to you impossible music <UNK> <UNK> and production production would\n",
      "Epoch: 50; Loss: 3.226551055908203; Sample: <S> often <UNK> of <UNK> being <UNK> its the <UNK> <UNK> an <UNK> and with </S> </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 51; Loss: 3.143051862716675; Sample: <S> whole <UNK> <UNK> <UNK> did next a <UNK> has the <UNK> <UNK> a <UNK> <UNK> a it one <UNK> ''\n",
      "Epoch: 52; Loss: 3.2187252044677734; Sample: <S> easily as <UNK> <UNK> story <UNK> <UNK> and <UNK> does may of <UNK> . , than the with <UNK> the\n",
      "Epoch: 53; Loss: 3.0781893730163574; Sample: <S> soap <UNK> . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 54; Loss: 3.2768166065216064; Sample: <S> humor <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55; Loss: 3.1482837200164795; Sample: <S> <UNK> <UNK> <UNK> narrative the <UNK> minutes here terrific <UNK> as <UNK> <UNK> of making <UNK> . </S> <PAD> <PAD>\n",
      "Epoch: 56; Loss: 3.1715989112854004; Sample: <S> to a in <UNK> will a an <UNK> -rrb- watching is fun has character <UNK> of going and <UNK> himself\n",
      "Epoch: 57; Loss: 3.211613178253174; Sample: <S> impossible power <UNK> but <UNK> <UNK> all crime almost well comedy like <UNK> is every <UNK> <UNK> turn likely you\n",
      "Epoch: 58; Loss: 3.1630446910858154; Sample: <S> days <UNK> <UNK> <UNK> <UNK> but <UNK> <UNK> of those and of the <UNK> <UNK> 's <UNK> is <UNK> <UNK>\n",
      "Epoch: 59; Loss: 3.154677152633667; Sample: <S> and <UNK> new `` <UNK> <UNK> is <UNK> 's <UNK> any : in <UNK> might his <UNK> . </S> <PAD>\n",
      "Epoch: 60; Loss: 3.171872854232788; Sample: <S> a about <UNK> melodrama <UNK> the film <UNK> <UNK> it french images it be the <UNK> <UNK> <UNK> <UNK> ,\n",
      "Epoch: 61; Loss: 3.0503907203674316; Sample: <S> you than about its <UNK> n't can and one and debut an <UNK> direction predictable the the <UNK> <UNK> ,\n",
      "Epoch: 62; Loss: 3.08613920211792; Sample: <S> without seeing up , <UNK> and probably of this <UNK> , <UNK> that one more life and a <UNK> light\n",
      "Epoch: 63; Loss: 3.1180665493011475; Sample: <S> <UNK> <UNK> the women spirit this end a <UNK> <UNK> <UNK> with year in to <UNK> comes <UNK> <UNK> by\n",
      "Epoch: 64; Loss: 3.068446636199951; Sample: <S> moment bit too to <UNK> his <UNK> who <UNK> up the action this <UNK> <UNK> and a <UNK> that <UNK>\n",
      "Epoch: 65; Loss: 3.1739766597747803; Sample: <S> you <UNK> , the <UNK> this <UNK> little is : that <UNK> of never are <UNK> , <UNK> and film\n",
      "Epoch: 66; Loss: 3.177495241165161; Sample: <S> a the <UNK> <UNK> who to <UNK> <UNK> is movies comedy <UNK> mostly <UNK> <UNK> does wit , a thought\n",
      "Epoch: 67; Loss: 3.325570583343506; Sample: <S> <UNK> is <UNK> <UNK> and many in never if <UNK> on predictable almost a a <UNK> filmmaking of <UNK> <UNK>\n",
      "Epoch: 68; Loss: 3.2644858360290527; Sample: <S> its <UNK> <UNK> of <UNK> of <UNK> of tone , , <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 69; Loss: 3.1158032417297363; Sample: <S> of next <UNK> as much a more laughs in <UNK> <UNK> before mystery is <UNK> many actor most who <UNK>\n",
      "Epoch: 70; Loss: 3.1088621616363525; Sample: <S> <UNK> attempt and about <UNK> is short that one of <UNK> film <UNK> <UNK> <UNK> much for jokes <UNK> <UNK>\n",
      "Epoch: 71; Loss: 3.0950539112091064; Sample: <S> on as a then <UNK> the <UNK> of <UNK> <UNK> a ' that beautiful , <UNK> . </S> <PAD> <PAD>\n",
      "Epoch: 72; Loss: 3.0923142433166504; Sample: <S> piece <UNK> is a the <UNK> bad that and its <UNK> yet . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 73; Loss: 3.289555311203003; Sample: <S> also was with entertaining 's ends mostly and they lives and interesting there , will really . </S> <PAD> <PAD>\n",
      "Epoch: 74; Loss: 2.91725492477417; Sample: <S> film . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 75; Loss: 3.034284830093384; Sample: <S> in lives it <UNK> made of best and cinematic face <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 76; Loss: 3.21801495552063; Sample: <S> effort <UNK> , soap <UNK> <UNK> in he . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 77; Loss: 3.1776928901672363; Sample: <S> some <UNK> to the <UNK> looks made <UNK> the the the <UNK> <UNK> of than for the <UNK> and <UNK>\n",
      "Epoch: 78; Loss: 3.095763921737671; Sample: <S> as <UNK> get black one can <UNK> 's far in too a reality , may <UNK> , comedy . </S>\n",
      "Epoch: 79; Loss: 3.094550609588623; Sample: <S> are <UNK> <UNK> with a <UNK> how , <UNK> and <UNK> at <UNK> heart are <UNK> with does and the\n",
      "Epoch: 80; Loss: 3.074176073074341; Sample: <S> <UNK> is human <UNK> , book <UNK> <UNK> , <UNK> to <UNK> ' its call , <UNK> with a <UNK>\n",
      "Epoch: 81; Loss: 3.0954225063323975; Sample: <S> her the best is <UNK> neither , are the <UNK> thriller , world <UNK> </S> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 82; Loss: 3.0194129943847656; Sample: <S> <UNK> most <UNK> <UNK> and dark <UNK> -rrb- <UNK> to nearly <UNK> character ... that the quite <UNK> this as\n",
      "Epoch: 83; Loss: 3.110743999481201; Sample: <S> the <UNK> <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 84; Loss: 3.089578628540039; Sample: <S> next the original <UNK> and <UNK> of <UNK> just in by the <UNK> sequences <UNK> <UNK> <UNK> least not plot\n",
      "Epoch: 85; Loss: 3.013892412185669; Sample: <S> the <UNK> , entirely <UNK> <UNK> and <UNK> <UNK> to <UNK> is is solid out so . </S> <PAD> <PAD>\n",
      "Epoch: 86; Loss: 3.0472609996795654; Sample: <S> watching <UNK> comedy -lrb- <UNK> documentary of <UNK> even <UNK> to <UNK> of <UNK> , <UNK> <UNK> <UNK> a <UNK>\n",
      "Epoch: 87; Loss: 3.2064297199249268; Sample: <S> exercise of one , has given to <UNK> by should a <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 88; Loss: 3.17287278175354; Sample: <S> as <UNK> is a <UNK> <UNK> of the drama may <UNK> to makes <UNK> effort <UNK> 're minutes 's <UNK>\n",
      "Epoch: 89; Loss: 3.2354800701141357; Sample: <S> it has off in cliches charm some <UNK> by tale <UNK> on <UNK> and <UNK> to <UNK> kids to <UNK>\n",
      "Epoch: 90; Loss: 3.0419883728027344; Sample: <S> ways clever there . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 91; Loss: 3.113131046295166; Sample: <S> <UNK> tv along <UNK> visual <UNK> be rich 's <UNK> an <UNK> <UNK> , <UNK> , <UNK> gags art about\n",
      "Epoch: 92; Loss: 3.025858163833618; Sample: <S> <UNK> <UNK> going and <UNK> to the <UNK> <UNK> , <UNK> who film that so <UNK> n't <UNK> the the\n",
      "Epoch: 93; Loss: 2.999133586883545; Sample: <S> performance it 's ... 's <UNK> 's that <UNK> are surprising , funny , to we the intelligent it ,\n",
      "Epoch: 94; Loss: 3.1891565322875977; Sample: <S> character , be <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 95; Loss: 3.1502091884613037; Sample: <S> <UNK> of fine <UNK> <UNK> <UNK> <UNK> surprisingly <UNK> as <UNK> does makes out <UNK> to <UNK> <UNK> and enjoy\n",
      "Epoch: 96; Loss: 3.066849946975708; Sample: <S> face <UNK> is <UNK> to a <UNK> performance <UNK> and for <UNK> drama compelling 's but turns n't all the\n",
      "Epoch: 97; Loss: 3.149808168411255; Sample: <S> acting it enough <UNK> for this <UNK> that n't <UNK> music . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 98; Loss: 3.0643105506896973; Sample: <S> filmmaking he of it on <UNK> ! </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 99; Loss: 3.094534397125244; Sample: <S> predictable movies to i <UNK> <UNK> as the <UNK> <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 100; Loss: 3.2072761058807373; Sample: <S> looking an movies , it 's <UNK> <UNK> are the <UNK> to <UNK> of <UNK> <UNK> <UNK> . . <PAD>\n",
      "Epoch: 101; Loss: 3.093168020248413; Sample: <S> <UNK> . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 102; Loss: 3.110637664794922; Sample: <S> should <UNK> a <UNK> i comes <UNK> to the <UNK> <UNK> would <UNK> and <UNK> , a <UNK> tale ,\n",
      "Epoch: 103; Loss: 3.1670165061950684; Sample: <S> the gives familiar thriller . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 104; Loss: 3.162651300430298; Sample: <S> same personal will ! truly is new bad bit , n't but <UNK> of fascinating two that <UNK> <UNK> of\n",
      "Epoch: 105; Loss: 3.031332492828369; Sample: <S> <UNK> <UNK> <UNK> and <UNK> time , <UNK> does some , a <UNK> seen <UNK> to <UNK> <UNK> and there\n",
      "Epoch: 106; Loss: 3.1932177543640137; Sample: <S> he like all the only <UNK> the <UNK> drama , the <UNK> <UNK> may need `` <UNK> to <UNK> to\n",
      "Epoch: 107; Loss: 3.124539613723755; Sample: <S> <UNK> <UNK> has genre the <UNK> as <UNK> , <UNK> quite already of <UNK> is feel a very <UNK> ,\n",
      "Epoch: 108; Loss: 3.143193483352661; Sample: <S> <UNK> and this children as what in any her a film script , the <UNK> the <UNK> and whose <UNK>\n",
      "Epoch: 109; Loss: 3.113576889038086; Sample: <S> look is compelling the <UNK> of only <UNK> more lacks to the <UNK> , is director <UNK> <UNK> by but\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110; Loss: 3.104783058166504; Sample: <S> <UNK> fresh '' <UNK> modern <UNK> and <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 111; Loss: 3.021033763885498; Sample: <S> yet a <UNK> 's short ... . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 112; Loss: 3.0972237586975098; Sample: <S> black funny is getting one . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 113; Loss: 2.9712319374084473; Sample: <S> director women some with <UNK> <UNK> <UNK> and anyone <UNK> here <UNK> for as the no has <UNK> <UNK> of\n",
      "Epoch: 114; Loss: 2.9188807010650635; Sample: <S> remains yet the <UNK> -rrb- <UNK> <UNK> that <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 115; Loss: 2.947082042694092; Sample: <S> they it 's <UNK> look a <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> left <UNK> <UNK> on the <UNK>\n",
      "Epoch: 116; Loss: 2.993809700012207; Sample: <S> know do <UNK> the <UNK> <UNK> and everyone contrived that to <UNK> , what go up be even <UNK> `\n",
      "Epoch: 117; Loss: 2.9534449577331543; Sample: <S> minutes of <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 118; Loss: 3.071688175201416; Sample: <S> <UNK> <UNK> with <UNK> film and our . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 119; Loss: 2.8777987957000732; Sample: <S> subject <UNK> the been <UNK> of long for young <UNK> and a <UNK> <UNK> <UNK> , <UNK> that <UNK> <UNK>\n",
      "Epoch: 120; Loss: 3.101728916168213; Sample: <S> guys and <UNK> is <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 121; Loss: 3.071897029876709; Sample: <S> it in <UNK> <UNK> is characters <UNK> of <UNK> <UNK> tired . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 122; Loss: 3.025804281234741; Sample: <S> it 's <UNK> more audience life like <UNK> <UNK> that a much 's <UNK> <UNK> or movies offers <UNK> <UNK>\n",
      "Epoch: 123; Loss: 3.088470697402954; Sample: <S> ... it . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 124; Loss: 3.0591559410095215; Sample: <S> worth <UNK> but the director <UNK> , long 's a good ... leave instead <UNK> . </S> <PAD> <PAD> <PAD>\n",
      "Epoch: 125; Loss: 2.9041526317596436; Sample: <S> visually what <UNK> is all by full that 're need history 's <UNK> <UNK> with way on , a not\n",
      "Epoch: 126; Loss: 3.0379130840301514; Sample: <S> , that 's <UNK> `` <UNK> is <UNK> that by the difficult <UNK> <UNK> <UNK> as time <UNK> the <UNK>\n",
      "Epoch: 127; Loss: 3.068491220474243; Sample: <S> of the sense , into , <UNK> to <UNK> <UNK> , a up in <UNK> <UNK> can <UNK> by <UNK>\n",
      "Epoch: 128; Loss: 3.030177593231201; Sample: <S> <UNK> , the <UNK> of <UNK> of gets <UNK> by <UNK> and <UNK> from like <UNK> . 's time the\n",
      "Epoch: 129; Loss: 3.081714153289795; Sample: <S> spirit to pretentious care since and <UNK> and big <UNK> <UNK> to the there little a <UNK> that <UNK> goes\n",
      "Epoch: 130; Loss: 3.046973705291748; Sample: <S> the a <UNK> this and <UNK> that by <UNK> with moments , <UNK> both only difficult by know <UNK> the\n",
      "Epoch: 131; Loss: 3.1365504264831543; Sample: <S> it gags has the reason <UNK> all of <UNK> is -lrb- <UNK> , do the end about <UNK> that ca\n",
      "Epoch: 132; Loss: 3.0550739765167236; Sample: <S> this <UNK> , and <UNK> of clever with <UNK> ; summer film the <UNK> than <UNK> too <UNK> 's <UNK>\n",
      "Epoch: 133; Loss: 2.8394582271575928; Sample: <S> the film a the give <UNK> <UNK> out , <UNK> <UNK> 've of formula at been <UNK> . </S> <PAD>\n",
      "Epoch: 134; Loss: 3.051647424697876; Sample: <S> a <UNK> <UNK> , <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 135; Loss: 3.0566155910491943; Sample: <S> the <UNK> <UNK> ! , <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 136; Loss: 3.1371912956237793; Sample: <S> although you a <UNK> directed 's <UNK> and and <UNK> , not a <UNK> <UNK> . </S> <PAD> <PAD> <PAD>\n",
      "Epoch: 137; Loss: 3.0962843894958496; Sample: <S> <UNK> , <UNK> <UNK> <UNK> you <UNK> we a characters plays <UNK> of <UNK> dark a <UNK> heart <UNK> its\n",
      "Epoch: 138; Loss: 3.1149396896362305; Sample: <S> lost only is to performances mystery to <UNK> <UNK> of who <UNK> very <UNK> <UNK> and which as some which\n",
      "Epoch: 139; Loss: 2.9268925189971924; Sample: <S> hilarious day in <UNK> and but a best the movie <UNK> , make any <UNK> <UNK> but <UNK> <UNK> <UNK>\n",
      "Epoch: 140; Loss: 3.0020084381103516; Sample: <S> and do family premise , it because <UNK> less . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 141; Loss: 3.082493305206299; Sample: <S> the familiar <UNK> becomes the <UNK> and <UNK> to get . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 142; Loss: 3.143157482147217; Sample: <S> there probably <UNK> but star all many do feature ... n't is <UNK> video <UNK> feels like an <UNK> will\n",
      "Epoch: 143; Loss: 2.907437801361084; Sample: <S> thriller to a <UNK> that this n't <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 144; Loss: 2.974869728088379; Sample: <S> in small might <UNK> hollywood <UNK> with <UNK> , <UNK> is <UNK> <UNK> half <UNK> 's <UNK> and still .\n",
      "Epoch: 145; Loss: 3.106562852859497; Sample: <S> the role of <UNK> ... that ride a bad job is my <UNK> of it <UNK> <UNK> their down for\n",
      "Epoch: 146; Loss: 3.0874574184417725; Sample: <S> n't <UNK> it is just the <UNK> the film <UNK> to after ultimately <UNK> , the <UNK> <UNK> is cinematic\n",
      "Epoch: 147; Loss: 2.9793593883514404; Sample: <S> me is <UNK> <UNK> believe <UNK> <UNK> the <UNK> hollywood . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 148; Loss: 3.1330065727233887; Sample: <S> the filmmakers <UNK> <UNK> on a <UNK> <UNK> through ... <UNK> , . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 149; Loss: 3.0174999237060547; Sample: <S> <UNK> its picture <UNK> of the <UNK> <UNK> not <UNK> were its <UNK> and we to <UNK> on be .\n",
      "Epoch: 150; Loss: 2.9316556453704834; Sample: <S> the <UNK> <UNK> <UNK> and <UNK> ' <UNK> , <UNK> <UNK> -rrb- <UNK> , to film has small so cast\n",
      "Epoch: 151; Loss: 2.9429163932800293; Sample: <S> is no a <UNK> <UNK> -- , thing but and <UNK> <UNK> , ; <UNK> for to characters have <UNK>\n",
      "Epoch: 152; Loss: 2.9517452716827393; Sample: <S> he about <UNK> story mind and a <UNK> how <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 153; Loss: 3.0023369789123535; Sample: <S> the lives <UNK> <UNK> the <UNK> for the <UNK> comedy close . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 154; Loss: 2.9335455894470215; Sample: <S> war the watch of <UNK> humor is <UNK> hour it but dramatic the <UNK> <UNK> . </S> <PAD> <PAD> <PAD>\n",
      "Epoch: 155; Loss: 2.978224515914917; Sample: <S> who the romance with a <UNK> sweet <UNK> of the film left that n't put to <UNK> much -- ,\n",
      "Epoch: 156; Loss: 3.086313247680664; Sample: <S> the film on <UNK> with movies of <UNK> for a <UNK> and <UNK> special but events is watching beyond comic\n",
      "Epoch: 157; Loss: 3.1410374641418457; Sample: <S> high <UNK> <UNK> , <UNK> <UNK> the work of <UNK> you while <UNK> <UNK> personal and <UNK> still in this\n",
      "Epoch: 158; Loss: 3.049252986907959; Sample: <S> theater <UNK> we 's ever story and men is <UNK> still anyone make ` <UNK> that is not being their\n",
      "Epoch: 159; Loss: 2.940955638885498; Sample: <S> heart the an <UNK> again , their movie 's the <UNK> <UNK> are <UNK> <UNK> ' cast dialogue of an\n",
      "Epoch: 160; Loss: 3.0456228256225586; Sample: <S> <UNK> does me as ending this thing enjoy idea , it . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 161; Loss: 3.01383113861084; Sample: <S> for <UNK> to <UNK> <UNK> is <UNK> from <UNK> in too day of the <UNK> which trying 's the and\n",
      "Epoch: 162; Loss: 2.978177785873413; Sample: <S> <UNK> and <UNK> an looking of <UNK> that long <UNK> before whose <UNK> drama storytelling to the <UNK> , its\n",
      "Epoch: 163; Loss: 2.88435435295105; Sample: <S> a its , and <UNK> , a <UNK> a <UNK> of - <UNK> some <UNK> . </S> <PAD> <PAD> <PAD>\n",
      "Epoch: 164; Loss: 2.9233691692352295; Sample: <S> <UNK> , the best <UNK> <UNK> <UNK> <UNK> and a <UNK> <UNK> could problem has it is ` it 's\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165; Loss: 2.90327787399292; Sample: <S> this just comedy <UNK> fact and <UNK> a <UNK> a film like ride to <UNK> as be <UNK> in the\n",
      "Epoch: 166; Loss: 2.9766788482666016; Sample: <S> study these thing and its <UNK> <UNK> and history . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 167; Loss: 3.18192458152771; Sample: <S> probably <UNK> high compelling perfectly the <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 168; Loss: 2.8918492794036865; Sample: <S> <UNK> she of <UNK> an no to the film <UNK> 's <UNK> <UNK> he , <UNK> yet an <UNK> drama\n",
      "Epoch: 169; Loss: 3.0746452808380127; Sample: <S> a fascinating <UNK> will out and <UNK> , at <UNK> of the <UNK> , <UNK> who <UNK> <UNK> . </S>\n",
      "Epoch: 170; Loss: 3.0403997898101807; Sample: <S> nothing enjoy so <UNK> : more movie to message <UNK> into <UNK> 's <UNK> left , the <UNK> of <UNK>\n",
      "Epoch: 171; Loss: 2.940624952316284; Sample: <S> <UNK> gets . or opera . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 172; Loss: 2.930579900741577; Sample: <S> it the yet days <UNK> <UNK> is fact . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 173; Loss: 3.0296826362609863; Sample: <S> <UNK> of emotional <UNK> and <UNK> has finally her comes makes <UNK> are <UNK> is <UNK> audience , who to\n",
      "Epoch: 174; Loss: 3.0848641395568848; Sample: <S> the writing , than he has ride , <UNK> , this <UNK> is this to at just what then i\n",
      "Epoch: 175; Loss: 2.956814765930176; Sample: <S> it but n't <UNK> narrative my a <UNK> and at the into <UNK> <UNK> does be <UNK> to much 're\n",
      "Epoch: 176; Loss: 3.0589206218719482; Sample: <S> - <UNK> than the emotional <UNK> movie , only <UNK> is but romantic : the <UNK> wit <UNK> -rrb- in\n",
      "Epoch: 177; Loss: 3.0461080074310303; Sample: <S> too <UNK> <UNK> but <UNK> that <UNK> plays still performances entertaining is <UNK> becomes <UNK> <UNK> , 'm an a\n",
      "Epoch: 178; Loss: 3.1938719749450684; Sample: <S> action <UNK> is out no fun to <UNK> make the horror before <UNK> in the <UNK> <UNK> for the -rrb-\n",
      "Epoch: 179; Loss: 3.0265557765960693; Sample: <S> <UNK> that is a <UNK> of <UNK> <UNK> and intelligence <UNK> , that the <UNK> <UNK> <UNK> and the kind\n",
      "Epoch: 180; Loss: 2.8488638401031494; Sample: <S> completely <UNK> is then <UNK> a <UNK> <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 181; Loss: 2.9856510162353516; Sample: <S> the fails are <UNK> <UNK> an more many <UNK> <UNK> as its film are <UNK> <UNK> . </S> <PAD> <PAD>\n",
      "Epoch: 182; Loss: 3.070892810821533; Sample: <S> -lrb- while <UNK> half may almost <UNK> falls , <UNK> and it <UNK> intriguing <UNK> and as <UNK> <UNK> ,\n",
      "Epoch: 183; Loss: 2.8896431922912598; Sample: <S> <UNK> is they character goes performance , a <UNK> ... nothing <UNK> than <UNK> play is <UNK> than the <UNK>\n",
      "Epoch: 184; Loss: 3.078967332839966; Sample: <S> nearly <UNK> <UNK> of <UNK> more entertainment <UNK> from if the <UNK> <UNK> <UNK> <UNK> , <UNK> is the <UNK>\n",
      "Epoch: 185; Loss: 3.027596950531006; Sample: <S> to a <UNK> '' that fun . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 186; Loss: 2.9700961112976074; Sample: <S> <UNK> is a performance ii <UNK> <UNK> <UNK> by <UNK> for the see <UNK> . </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 187; Loss: 3.0314154624938965; Sample: <S> personal <UNK> <UNK> <UNK> ' moving project : a <UNK> <UNK> <UNK> how <UNK> at <UNK> to <UNK> <UNK> .\n",
      "Epoch: 188; Loss: 3.0635180473327637; Sample: <S> if are <UNK> humor in <UNK> <UNK> 's the <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 189; Loss: 2.9793660640716553; Sample: <S> but <UNK> a little , <UNK> not over . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 190; Loss: 3.0172955989837646; Sample: <S> the all <UNK> much . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 191; Loss: 2.9802963733673096; Sample: <S> in <UNK> 's <UNK> adventure </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 192; Loss: 2.877570629119873; Sample: <S> , you <UNK> at the <UNK> , as its <UNK> and the <UNK> <UNK> <UNK> the movie -rrb- wrong to\n",
      "Epoch: 193; Loss: 3.023292064666748; Sample: <S> <UNK> an movie <UNK> <UNK> , script thing 's star were more <UNK> this <UNK> <UNK> , . </S> <PAD>\n",
      "Epoch: 194; Loss: 2.97265887260437; Sample: <S> <UNK> is <UNK> , an <UNK> character <UNK> solid , the screen that 's <UNK> , them , <UNK> old\n",
      "Epoch: 195; Loss: 3.089555263519287; Sample: <S> <UNK> , <UNK> too plot of a <UNK> is a <UNK> of the <UNK> of the story <UNK> , <UNK>\n",
      "Epoch: 196; Loss: 2.968003749847412; Sample: <S> it , a <UNK> in <UNK> and <UNK> because <UNK> <UNK> at <UNK> summer . </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 197; Loss: 2.8364529609680176; Sample: <S> an <UNK> of the kind of <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 198; Loss: 3.03684139251709; Sample: <S> good <UNK> <UNK> <UNK> may with a movie worst <UNK> and <UNK> <UNK> <UNK> , . </S> <PAD> <PAD> <PAD>\n",
      "Epoch: 199; Loss: 2.9767568111419678; Sample: <S> ... <UNK> -lrb- humor to most <UNK> without <UNK> your plot and the one , <UNK> <UNK> none to hilarious\n",
      "Epoch: 200; Loss: 2.918431043624878; Sample: <S> the <UNK> coming-of-age <UNK> <UNK> also <UNK> , <UNK> a <UNK> of the <UNK> hard but history that home or\n",
      "Epoch: 201; Loss: 2.9061360359191895; Sample: <S> down comedy to directed to being <UNK> directed <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 202; Loss: 2.994873285293579; Sample: <S> , <UNK> is n't writing when into <UNK> to <UNK> , <UNK> comedy <UNK> , this in <UNK> <UNK> is\n",
      "Epoch: 203; Loss: 3.0189433097839355; Sample: <S> a other <UNK> documentary of <UNK> ? </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 204; Loss: 2.8857033252716064; Sample: <S> if people <UNK> to only <UNK> are to a its imagination serious <UNK> dark them `` turns to for <UNK>\n",
      "Epoch: 205; Loss: 3.005007266998291; Sample: <S> <UNK> with a film visual laughs and little very good <UNK> here and as a how <UNK> . </S> <PAD>\n",
      "Epoch: 206; Loss: 2.887460947036743; Sample: <S> does despite <UNK> <UNK> exercise who <UNK> its her <UNK> makes <UNK> is without the <UNK> <UNK> in enough <UNK>\n",
      "Epoch: 207; Loss: 2.9264936447143555; Sample: <S> from <UNK> <UNK> on the <UNK> reality with its few <UNK> <UNK> any about it as as tale - first\n",
      "Epoch: 208; Loss: 2.981037139892578; Sample: <S> the <UNK> movie in <UNK> with the still about <UNK> performances many , the a <UNK> <UNK> themselves 's <UNK>\n",
      "Epoch: 209; Loss: 2.918872833251953; Sample: <S> and <UNK> an years if who <UNK> is <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 210; Loss: 2.9503273963928223; Sample: <S> like <UNK> to days after <UNK> are <UNK> that because <UNK> <UNK> <UNK> the <UNK> at the <UNK> face from\n",
      "Epoch: 211; Loss: 2.991145133972168; Sample: <S> if did how being it , this moment always not <UNK> seem , <UNK> <UNK> , <UNK> of <UNK> in\n",
      "Epoch: 212; Loss: 3.0343546867370605; Sample: <S> but i see <UNK> only <UNK> women <UNK> <UNK> if by <UNK> , <UNK> away and <UNK> the <UNK> .\n",
      "Epoch: 213; Loss: 2.960538387298584; Sample: <S> its <UNK> and what as in <UNK> story of storytelling and <UNK> on the <UNK> as lack of the <UNK>\n",
      "Epoch: 214; Loss: 3.0114150047302246; Sample: <S> the big <UNK> has <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 215; Loss: 2.993955135345459; Sample: <S> <UNK> he the none of no <UNK> <UNK> years <UNK> quite even <UNK> <UNK> things , <UNK> . </S> <PAD>\n",
      "Epoch: 216; Loss: 2.9259419441223145; Sample: <S> was <UNK> -- with the <UNK> that <UNK> you good of it intelligence <UNK> watch and tired in <UNK> that\n",
      "Epoch: 217; Loss: 3.0666110515594482; Sample: <S> the comedy performances -rrb- for the <UNK> <UNK> to <UNK> about <UNK> <UNK> of <UNK> very <UNK> just comedy to\n",
      "Epoch: 218; Loss: 2.830876588821411; Sample: <S> <UNK> there the make yet that <UNK> -rrb- about <UNK> 's american <UNK> to almost romantic . </S> <PAD> <PAD>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219; Loss: 2.964733362197876; Sample: <S> filmmaker too could dramatic <UNK> that <UNK> 's we comes by none of the <UNK> during <UNK> . </S> <PAD>\n",
      "Epoch: 220; Loss: 2.8489484786987305; Sample: <S> a <UNK> that <UNK> <UNK> <UNK> as if us <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 221; Loss: 3.0403082370758057; Sample: <S> sure been cast <UNK> that want to be the <UNK> and <UNK> he </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 222; Loss: 2.8653080463409424; Sample: <S> a <UNK> the who if <UNK> director scenes , a <UNK> in <UNK> , 's video <UNK> that years it\n",
      "Epoch: 223; Loss: 2.8731276988983154; Sample: <S> and action <UNK> <UNK> in a <UNK> more <UNK> <UNK> <UNK> <UNK> '' into <UNK> <UNK> film have tries would\n",
      "Epoch: 224; Loss: 2.9928526878356934; Sample: <S> the people <UNK> as the <UNK> are of <UNK> <UNK> and <UNK> <UNK> of the <UNK> <UNK> <UNK> to <UNK>\n",
      "Epoch: 225; Loss: 2.856098175048828; Sample: <S> not <UNK> into its <UNK> thought with better comic <UNK> ' to love and , , from his film on\n",
      "Epoch: 226; Loss: 3.0655517578125; Sample: <S> <UNK> 's <UNK> <UNK> <UNK> to actors , a <UNK> <UNK> <UNK> does a -rrb- comic <UNK> , come everyone\n",
      "Epoch: 227; Loss: 2.880922794342041; Sample: <S> better such <UNK> <UNK> and <UNK> , <UNK> amusing and . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 228; Loss: 3.094320297241211; Sample: <S> the film 's <UNK> <UNK> , <UNK> up 's <UNK> direction </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 229; Loss: 3.05733060836792; Sample: <S> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 230; Loss: 2.841862201690674; Sample: <S> might this <UNK> is the kind of <UNK> , when <UNK> 's <UNK> <UNK> this beautifully <UNK> is from heart\n",
      "Epoch: 231; Loss: 3.0243101119995117; Sample: <S> and a <UNK> world and <UNK> particularly when no ` woman . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 232; Loss: 2.880932569503784; Sample: <S> <UNK> above i take a sex <UNK> tragedy are worst think . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 233; Loss: 3.0464487075805664; Sample: <S> that a <UNK> narrative think in the dialogue of <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 234; Loss: 2.9545936584472656; Sample: <S> but sort over <UNK> know or <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 235; Loss: 2.8788440227508545; Sample: <S> in a <UNK> <UNK> <UNK> in <UNK> <UNK> 's <UNK> same <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 236; Loss: 3.023226737976074; Sample: <S> <UNK> <UNK> <UNK> we <UNK> <UNK> proves would <UNK> people , <UNK> still , <UNK> , <UNK> <UNK> . </S>\n",
      "Epoch: 237; Loss: 3.037654399871826; Sample: <S> a <UNK> experience of <UNK> and humor and <UNK> no <UNK> to <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 238; Loss: 2.9700307846069336; Sample: <S> or often , i <UNK> star material of memorable <UNK> how <UNK> no <UNK> from <UNK> your <UNK> , <UNK>\n",
      "Epoch: 239; Loss: 2.907864809036255; Sample: <S> it <UNK> in its against has the little <UNK> for character is <UNK> the war of <UNK> , more <UNK>\n",
      "Epoch: 240; Loss: 3.0244622230529785; Sample: <S> the <UNK> of you has a characters energy <UNK> 's <UNK> <UNK> , kind , but <UNK> little <UNK> <UNK>\n",
      "Epoch: 241; Loss: 2.9546661376953125; Sample: <S> a <UNK> <UNK> <UNK> 's women <UNK> is <UNK> to <UNK> as a <UNK> , as the shot but <UNK>\n",
      "Epoch: 242; Loss: 2.909898519515991; Sample: <S> it new out to a <UNK> they <UNK> his <UNK> <UNK> of <UNK> <UNK> <UNK> his <UNK> . </S> <PAD>\n",
      "Epoch: 243; Loss: 2.909846305847168; Sample: <S> <UNK> <UNK> engaging <UNK> , <UNK> plays <UNK> as the rather <UNK> he <UNK> <UNK> is <UNK> , <UNK> the\n",
      "Epoch: 244; Loss: 2.959355115890503; Sample: <S> all <UNK> <UNK> makes <UNK> music ... 's a was <UNK> as , like so <UNK> . </S> <PAD> <PAD>\n",
      "Epoch: 245; Loss: 2.926862955093384; Sample: <S> i <UNK> a man direction , is all ... will too could off <UNK> to no never <UNK> <UNK> with\n",
      "Epoch: 246; Loss: 2.925178050994873; Sample: <S> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 247; Loss: 2.961310863494873; Sample: <S> you 'll not <UNK> to sequences for portrait is <UNK> , about imagination <UNK> and those pleasure <UNK> and if\n",
      "Epoch: 248; Loss: 2.9614675045013428; Sample: <S> <UNK> is <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 249; Loss: 2.9317469596862793; Sample: <S> a kind is cold him and all things , but and the <UNK> in death 's and <UNK> and own\n",
      "Epoch: 250; Loss: 3.0161080360412598; Sample: <S> ... finally should he your <UNK> <UNK> wo <UNK> -- but the <UNK> to like <UNK> <UNK> as <UNK> ''\n",
      "Epoch: 251; Loss: 2.969416856765747; Sample: <S> its <UNK> <UNK> <UNK> <UNK> and the moving of the material . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 252; Loss: 2.952514410018921; Sample: <S> <UNK> this 's while <UNK> effort and their us . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 253; Loss: 2.8587076663970947; Sample: <S> a new <UNK> makes he things than it is <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 254; Loss: 2.969189167022705; Sample: <S> <UNK> the material movie is life gives a <UNK> place . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 255; Loss: 2.9979987144470215; Sample: <S> it ' <UNK> <UNK> mostly a <UNK> <UNK> nearly much far : now , that <UNK> a <UNK> and <UNK>\n",
      "Epoch: 256; Loss: 3.0285496711730957; Sample: <S> the <UNK> of neither likely to its <UNK> <UNK> with after <UNK> its <UNK> way . </S> <PAD> <PAD> <PAD>\n",
      "Epoch: 257; Loss: 2.998713970184326; Sample: <S> that can be gives <UNK> <UNK> is been <UNK> <UNK> and all it <UNK> a show : new wrong on\n",
      "Epoch: 258; Loss: 2.929921865463257; Sample: <S> i suspense . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 259; Loss: 2.894148826599121; Sample: <S> men is debut lost project one to <UNK> and than <UNK> this <UNK> ? is any a movie the <UNK>\n",
      "Epoch: 260; Loss: 2.822237253189087; Sample: <S> a <UNK> <UNK> the director and <UNK> sad of <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 261; Loss: 2.9286885261535645; Sample: <S> you comes would see too <UNK> <UNK> find into the movie , i he he up the together . </S>\n",
      "Epoch: 262; Loss: 2.9769816398620605; Sample: <S> <UNK> each by <UNK> <UNK> of more good <UNK> <UNK> <UNK> , as <UNK> , but <UNK> takes this and\n",
      "Epoch: 263; Loss: 2.881136417388916; Sample: <S> a <UNK> , it 's <UNK> an <UNK> <UNK> <UNK> 's a <UNK> , <UNK> all <UNK> , the <UNK>\n",
      "Epoch: 264; Loss: 3.053715705871582; Sample: <S> quirky with the <UNK> it <UNK> does be you that <UNK> high <UNK> of and films worth beautifully <UNK> ,\n",
      "Epoch: 265; Loss: 2.9481728076934814; Sample: <S> when <UNK> but <UNK> n't ultimately <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 266; Loss: 2.993758201599121; Sample: <S> out their lot , <UNK> <UNK> <UNK> ... too making <UNK> again of portrait funny . </S> <PAD> <PAD> <PAD>\n",
      "Epoch: 267; Loss: 3.0426981449127197; Sample: <S> ultimately <UNK> the de movie 's <UNK> , <UNK> , at ` <UNK> <UNK> <UNK> 's <UNK> <UNK> -rrb- a\n",
      "Epoch: 268; Loss: 2.973939895629883; Sample: <S> you 's for <UNK> the <UNK> 's a great <UNK> is a american <UNK> <UNK> who your romantic <UNK> .\n",
      "Epoch: 269; Loss: 2.852072238922119; Sample: <S> <UNK> the <UNK> <UNK> is a little <UNK> only even the <UNK> from the thriller <UNK> <UNK> that <UNK> <UNK>\n",
      "Epoch: 270; Loss: 2.8956074714660645; Sample: <S> day and <UNK> and movies , both a <UNK> long , <UNK> , <UNK> from <UNK> <UNK> . </S> <PAD>\n",
      "Epoch: 271; Loss: 2.935288190841675; Sample: <S> it is <UNK> on looks of around <UNK> of a <UNK> <UNK> clever <UNK> , the movie 's <UNK> flat\n",
      "Epoch: 272; Loss: 2.884824275970459; Sample: <S> there 's an heart <UNK> is good of all as <UNK> effects , <UNK> <UNK> the which to actors into\n",
      "Epoch: 273; Loss: 2.9002392292022705; Sample: <S> has a soap two <UNK> films surprisingly of <UNK> of <UNK> between <UNK> , <UNK> believe . </S> <PAD> <PAD>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274; Loss: 2.8405888080596924; Sample: <S> n't <UNK> coming-of-age <UNK> with see interesting to new film is n't of still <UNK> <UNK> to his very <UNK>\n",
      "Epoch: 275; Loss: 2.9528439044952393; Sample: <S> <UNK> so <UNK> , the role of <UNK> to make a <UNK> 's the thought <UNK> <UNK> does its its\n",
      "Epoch: 276; Loss: 2.828068494796753; Sample: <S> <UNK> has have <UNK> is by <UNK> has <UNK> a <UNK> into the <UNK> <UNK> could <UNK> over as music\n",
      "Epoch: 277; Loss: 2.854419708251953; Sample: <S> <UNK> <UNK> <UNK> in <UNK> , the <UNK> for the smart of <UNK> <UNK> to <UNK> dumb . </S> <PAD>\n",
      "Epoch: 278; Loss: 2.9468915462493896; Sample: <S> seems narrative the <UNK> ... directed <UNK> <UNK> through <UNK> <UNK> enjoyable of the <UNK> show of <UNK> and <UNK>\n",
      "Epoch: 279; Loss: 2.923605442047119; Sample: <S> <UNK> , there 's time is a <UNK> of the <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 280; Loss: 3.0577080249786377; Sample: <S> <UNK> <UNK> <UNK> funny in your <UNK> of <UNK> and <UNK> <UNK> who never which be a <UNK> <UNK> ,\n",
      "Epoch: 281; Loss: 2.9876720905303955; Sample: <S> film <UNK> <UNK> or a fascinating and <UNK> <UNK> , you else that be down about a <UNK> <UNK> <UNK>\n",
      "Epoch: 282; Loss: 2.7903389930725098; Sample: <S> a <UNK> <UNK> and <UNK> -rrb- fascinating narrative and a <UNK> , <UNK> as <UNK> has any <UNK> the characters\n",
      "Epoch: 283; Loss: 3.012514591217041; Sample: <S> a lack <UNK> may <UNK> <UNK> , nearly <UNK> like <UNK> 's <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 284; Loss: 2.903069496154785; Sample: <S> it i can being <UNK> is its <UNK> comic . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 285; Loss: 2.8873276710510254; Sample: <S> it or be sweet <UNK> , in <UNK> 's <UNK> and the away 's <UNK> <UNK> tone . </S> <PAD>\n",
      "Epoch: 286; Loss: 2.9705986976623535; Sample: <S> is <UNK> <UNK> and good but what message , a <UNK> <UNK> and right <UNK> , from <UNK> <UNK> 'll\n",
      "Epoch: 287; Loss: 3.0000908374786377; Sample: <S> under i perfect <UNK> <UNK> to good -- of brilliant : despite <UNK> works enjoy this <UNK> but 's <UNK>\n",
      "Epoch: 288; Loss: 2.8219895362854004; Sample: <S> , the good <UNK> where for <UNK> the <UNK> sad know . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 289; Loss: 2.8525052070617676; Sample: <S> <UNK> part a rather <UNK> , <UNK> of it <UNK> a time me </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 290; Loss: 2.9787747859954834; Sample: <S> you ii simply <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 291; Loss: 2.7250730991363525; Sample: <S> a movie <UNK> like <UNK> <UNK> and actually have not compelling that another one . </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 292; Loss: 2.9666361808776855; Sample: <S> flick 's at a <UNK> of the comedy to in the <UNK> pretentious . </S> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 293; Loss: 2.88881254196167; Sample: <S> -lrb- i <UNK> an <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 294; Loss: 3.101496458053589; Sample: <S> his a <UNK> <UNK> plot to ca a <UNK> about <UNK> , , after a <UNK> of a <UNK> john\n",
      "Epoch: 295; Loss: 2.8407304286956787; Sample: <S> <UNK> the movie given <UNK> <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 296; Loss: 2.9283769130706787; Sample: <S> himself over <UNK> and get satire , shot and good romantic <UNK> in <UNK> . </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 297; Loss: 2.934807300567627; Sample: <S> 's many <UNK> and just <UNK> to be i <UNK> that <UNK> in mind me <UNK> about enough that in\n",
      "Epoch: 298; Loss: 2.915541648864746; Sample: <S> this <UNK> performance of <UNK> she is <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 299; Loss: 2.935659885406494; Sample: <S> a women of <UNK> should <UNK> is either in the <UNK> of bad <UNK> for a <UNK> <UNK> about -lrb-\n",
      "Epoch: 300; Loss: 3.039494752883911; Sample: <S> if it delivers must n't what up the video is through been <UNK> above that <UNK> despite <UNK> is character\n",
      "Epoch: 301; Loss: 2.8818886280059814; Sample: <S> day <UNK> <UNK> <UNK> <UNK> to enough through and <UNK> <UNK> the action <UNK> , is <UNK> of reason should\n",
      "Epoch: 302; Loss: 2.934868335723877; Sample: <S> <UNK> with real <UNK> , cinema moments idea . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 303; Loss: 3.023393154144287; Sample: <S> this , who <UNK> in the different and <UNK> of <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 304; Loss: 2.9208271503448486; Sample: <S> ... <UNK> a script see you has this <UNK> it is be like and the <UNK> comedy <UNK> . </S>\n",
      "Epoch: 305; Loss: 3.0710041522979736; Sample: <S> something may the <UNK> <UNK> <UNK> never movie to <UNK> most <UNK> the <UNK> of the <UNK> <UNK> , the\n",
      "Epoch: 306; Loss: 2.91776967048645; Sample: <S> <UNK> to a look , the film 's two <UNK> screen in an <UNK> . </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 307; Loss: 3.066399574279785; Sample: <S> <UNK> 's <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 308; Loss: 2.913058042526245; Sample: <S> ca not like a <UNK> 'd high <UNK> <UNK> with <UNK> , clever and one of and filmmaker 's action\n",
      "Epoch: 309; Loss: 2.86588716506958; Sample: <S> as <UNK> laughs a <UNK> <UNK> is at not are , with <UNK> but this <UNK> <UNK> than the character\n",
      "Epoch: 310; Loss: 3.0654542446136475; Sample: <S> <UNK> is can a genre that it 's its <UNK> that 's <UNK> believe of dialogue '' <UNK> <UNK> we\n",
      "Epoch: 311; Loss: 2.939608335494995; Sample: <S> but i have a <UNK> <UNK> is level ? </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 312; Loss: 2.963487386703491; Sample: <S> if 's tragedy and <UNK> and <UNK> your <UNK> <UNK> enough <UNK> on <UNK> . </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 313; Loss: 2.8754801750183105; Sample: <S> feel even <UNK> is <UNK> , intelligence done , he 's <UNK> that as man works for <UNK> <UNK> of\n",
      "Epoch: 314; Loss: 2.794729709625244; Sample: <S> up the the <UNK> <UNK> of the <UNK> plenty production <UNK> for <UNK> , an who more <UNK> to <UNK>\n",
      "Epoch: 315; Loss: 2.950122117996216; Sample: <S> <UNK> from its <UNK> feeling , the <UNK> movie is a <UNK> did <UNK> <UNK> by <UNK> your <UNK> <UNK>\n",
      "Epoch: 316; Loss: 3.0072503089904785; Sample: <S> if you simply do n't <UNK> by the movie documentary in <UNK> wit , because power . </S> <PAD> <PAD>\n",
      "Epoch: 317; Loss: 2.91400408744812; Sample: <S> it do such the movie , something it and <UNK> ... <UNK> to a turn , but like no me\n",
      "Epoch: 318; Loss: 2.880906820297241; Sample: <S> the <UNK> 's <UNK> , he <UNK> movie no and fascinating come has one when not off <UNK> , a\n",
      "Epoch: 319; Loss: 2.868074655532837; Sample: <S> even no as <UNK> may making real <UNK> to a <UNK> , but <UNK> . </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 320; Loss: 2.942638635635376; Sample: <S> the <UNK> , horror is while <UNK> much , the movie with you <UNK> is <UNK> into <UNK> and <UNK>\n",
      "Epoch: 321; Loss: 3.008066177368164; Sample: <S> character <UNK> <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 322; Loss: 2.893779993057251; Sample: <S> the next , this are romance sense and <UNK> ... . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 323; Loss: 2.9610674381256104; Sample: <S> long they does take , ? </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 324; Loss: 2.8960201740264893; Sample: <S> what is <UNK> in one short <UNK> movie that 's makes back then <UNK> . </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 325; Loss: 2.853578567504883; Sample: <S> a formula <UNK> <UNK> he <UNK> funny for from <UNK> serious work that <UNK> that is the <UNK> in a\n",
      "Epoch: 326; Loss: 2.9060802459716797; Sample: <S> the <UNK> from time and days , violence , <UNK> , premise and of the engaging to <UNK> , ,\n",
      "Epoch: 327; Loss: 2.9958927631378174; Sample: <S> <UNK> , as <UNK> may be more american , <UNK> <UNK> , -- <UNK> <UNK> as <UNK> <UNK> into the\n",
      "Epoch: 328; Loss: 2.9319045543670654; Sample: <S> <UNK> comedy . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 329; Loss: 2.948920488357544; Sample: <S> if it is about <UNK> should have what in <UNK> <UNK> to <UNK> over on the whole <UNK> most <UNK>\n",
      "Epoch: 330; Loss: 2.873727321624756; Sample: <S> this <UNK> <UNK> , but <UNK> <UNK> 's <UNK> <UNK> of the movie that good <UNK> <UNK> <UNK> and a\n",
      "Epoch: 331; Loss: 2.897233009338379; Sample: <S> about but i a <UNK> of is he make quite a ends of those <UNK> comic but <UNK> from <UNK>\n",
      "Epoch: 332; Loss: 2.9485654830932617; Sample: <S> <UNK> <UNK> rather <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 333; Loss: 3.021697521209717; Sample: <S> and its <UNK> ` <UNK> for a <UNK> audiences <UNK> with very <UNK> french how <UNK> amusing to new sense\n",
      "Epoch: 334; Loss: 3.011765956878662; Sample: <S> <UNK> true viewers a <UNK> of <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 335; Loss: 2.9130468368530273; Sample: <S> . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 336; Loss: 2.8182835578918457; Sample: <S> <UNK> into the who got like <UNK> moments , <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 337; Loss: 2.8441200256347656; Sample: <S> the , <UNK> is <UNK> to my movie <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 338; Loss: 2.867363452911377; Sample: <S> <UNK> <UNK> could does out that over a <UNK> film , new <UNK> <UNK> <UNK> might every most <UNK> <UNK>\n",
      "Epoch: 339; Loss: 2.9041128158569336; Sample: <S> quite better remains <UNK> is the <UNK> characters that only so <UNK> as its original <UNK> , but be during\n",
      "Epoch: 340; Loss: 2.9624314308166504; Sample: <S> <UNK> <UNK> and <UNK> that a film <UNK> ... a <UNK> of which <UNK> fairly , the <UNK> , and\n",
      "Epoch: 341; Loss: 2.9665751457214355; Sample: <S> some the great way the all <UNK> <UNK> and me a <UNK> seems <UNK> wit <UNK> <UNK> . </S> <PAD>\n",
      "Epoch: 342; Loss: 2.9604039192199707; Sample: <S> this <UNK> play had charm about , it 's `` <UNK> <UNK> <UNK> with its film <UNK> to pretty remarkable\n",
      "Epoch: 343; Loss: 2.9512553215026855; Sample: <S> that an <UNK> <UNK> film at a <UNK> <UNK> of <UNK> its <UNK> <UNK> to too <UNK> you most has\n",
      "Epoch: 344; Loss: 2.914130687713623; Sample: <S> a most of <UNK> of <UNK> by <UNK> <UNK> of <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 345; Loss: 2.844771385192871; Sample: <S> the war of and much <UNK> , the <UNK> <UNK> <UNK> action <UNK> family that much much to his your\n",
      "Epoch: 346; Loss: 2.892112970352173; Sample: <S> you picture at with a <UNK> where every find never passion -- </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 347; Loss: 2.9288508892059326; Sample: <S> so title , <UNK> <UNK> of the world , old real <UNK> each <UNK> of special and <UNK> in the\n",
      "Epoch: 348; Loss: 2.956549644470215; Sample: <S> about the movie in <UNK> <UNK> kids , <UNK> but it be then and <UNK> that you you have make\n",
      "Epoch: 349; Loss: 2.875136137008667; Sample: <S> seem just it from a <UNK> <UNK> that i two small ' in every <UNK> the modern moments and the\n",
      "Epoch: 350; Loss: 2.9048314094543457; Sample: <S> if strange -rrb- <UNK> could video <UNK> who have the <UNK> <UNK> or take , <UNK> as <UNK> <UNK> back\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameters \n",
    "vocab_size = len(indices_to_words)\n",
    "seq_length = max_seq_length + 1\n",
    "hidden_dim = 32\n",
    "embedding_dim = 16\n",
    "batch_size = 256\n",
    "learning_rate = 0.3\n",
    "num_epochs = 350\n",
    "display_epoch_freq = 1\n",
    "\n",
    "# Build, initialize, and train model\n",
    "lm = RNNLM(vocab_size, seq_length, embedding_dim, hidden_dim, batch_size)\n",
    "lm.init_weights()\n",
    "\n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(lm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_iter = data_iter(training_set, batch_size)\n",
    "training_loop(batch_size, vocab_size, num_epochs, display_epoch_freq, lm, loss, optimizer, training_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Looking at the samples that your model produced towards the end of training, what properties of (written) English does it seem to have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** If we could make the model as big as we wanted, train as long as we wanted, and adjust or remove dropout at will, could we ever get the model to reach a cost value of 0.0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Would the model be any worse if we were to just delete unknown words instead of using an `<UNK>` token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras:\n",
    "\n",
    "* Do weight tying, i.e. reuse the input word embedding matrix as the output classification matrix. _https://arxiv.org/pdf/1611.01462.pdf_\n",
    "* Implement beam search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
